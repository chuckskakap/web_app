{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "MAX_SEQUENCE_LENGTH = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "loaded_model = keras.models.load_model(\"transformer_model\")\n",
    "\n",
    "# Load jw_vocab\n",
    "with open('jw_vocab.pkl', 'rb') as f:\n",
    "    jw_vocab = pickle.load(f)\n",
    "\n",
    "# Load rm_vocab\n",
    "with open('rm_vocab.pkl', 'rb') as f:\n",
    "    rm_vocab = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "jw_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=jw_vocab, lowercase=False\n",
    ")\n",
    "rm_tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
    "    vocabulary=rm_vocab, lowercase=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequences(input_sentences):\n",
    "    batch_size = tf.shape(input_sentences)[0]\n",
    "\n",
    "    # Tokenize the encoder input.\n",
    "    encoder_input_tokens = jw_tokenizer(input_sentences).to_tensor(\n",
    "        shape=(None, MAX_SEQUENCE_LENGTH)\n",
    "    )\n",
    "\n",
    "    # Define a function that outputs the next token's probability given the\n",
    "    # input sequence.\n",
    "    def next(prompt, cache, index):\n",
    "        logits = loaded_model([encoder_input_tokens, prompt])[:, index - 1, :]\n",
    "        # Ignore hidden states for now; only needed for contrastive search.\n",
    "        hidden_states = None\n",
    "        return logits, hidden_states, cache\n",
    "\n",
    "    # Build a prompt of length 40 with a start token and padding tokens.\n",
    "    length = 40\n",
    "    start = tf.fill((batch_size, 1), rm_tokenizer.token_to_id(\"[START]\"))\n",
    "    pad = tf.fill((batch_size, length - 1), rm_tokenizer.token_to_id(\"[PAD]\"))\n",
    "    prompt = tf.concat((start, pad), axis=-1)\n",
    "\n",
    "    generated_tokens = keras_nlp.samplers.GreedySampler()(\n",
    "        next,\n",
    "        prompt,\n",
    "        end_token_id=rm_tokenizer.token_to_id(\"[END]\"),\n",
    "        index=1,  # Start sampling after start token.\n",
    "    )\n",
    "    generated_sentences = rm_tokenizer.detokenize(generated_tokens)\n",
    "    return generated_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saya suka kamu\n",
      "nya jan\n"
     ]
    }
   ],
   "source": [
    "# input_sentence = 'ساي ساكيت ڤروت'\n",
    "# input_sentence = 'ساي هاڽ ڬاديس كامڤوڠ.'\n",
    "# input_sentence = 'اوق دڠكي'\n",
    "input_sentence = 'saya suka kamu'\n",
    "translated = decode_sequences(tf.constant([input_sentence]))\n",
    "translated = translated.numpy()[0].decode(\"utf-8\")\n",
    "translated = (\n",
    "    translated.replace(\"[PAD]\", \"\")\n",
    "    .replace(\"[START]\", \"\")\n",
    "    .replace(\"[END]\", \"\")\n",
    "    .strip()\n",
    ")\n",
    "\n",
    "print(input_sentence)\n",
    "print(translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.10.0\n",
      "2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(tf.keras.__version__)\n",
    "\n",
    "import keras\n",
    "\n",
    "print(keras.__version__)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
